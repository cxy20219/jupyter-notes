{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae83871",
   "metadata": {},
   "source": [
    "## 1. 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b03bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591441f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518032c6",
   "metadata": {},
   "source": [
    "### 张量初始化\n",
    "#### 1.直接生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c4ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b8597",
   "metadata": {},
   "source": [
    "#### 2.通过Numpy数组来生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e493a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "np_array = numpy.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf684d57",
   "metadata": {},
   "source": [
    "#### 3.通过已有的张量来生成新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0d03a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.3001, 0.4557],\n",
      "        [0.2706, 0.0643]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)   # 保留 x_data 的属性\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)   # 重写 x_data 的数据类型\n",
    "                                                      # int -> float\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35124494",
   "metadata": {},
   "source": [
    "#### 4.通过指定数据维度来生成张量\n",
    "shape是元组类型, 用来描述张量的维数, 下面3个函数通过传入shape来指定生成张量的维数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16e63317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.2934, 0.4323, 0.9440],\n",
      "        [0.4606, 0.9574, 0.0162]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#三维\n",
    "#shape = (2,3,1)\n",
    "#四维\n",
    "#shape = (2,3,1,1)\n",
    "#二维\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dac597",
   "metadata": {},
   "source": [
    "### 张量属性\n",
    "从张量属性我们可以得到张量的维数、数据类型以及它们所存储的设备(CPU或GPU)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18829f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7834, 0.5844, 0.6105, 0.2781],\n",
      "        [0.1261, 0.7820, 0.6283, 0.2514],\n",
      "        [0.9467, 0.8772, 0.9695, 0.4261]])\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(tensor)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7651ae",
   "metadata": {},
   "source": [
    "### 张量运算\n",
    "有超过100种张量相关的运算操作, 例如转置、索引、切片、数学运算、线性代数、随机采样等。更多的运算可以在[这里](https://pytorch.org/docs/stable/torch.html)查看。\n",
    "\n",
    "所有这些运算都可以在GPU上运行(相对于CPU来说可以达到更高的运算速度)。如果使用的是Google的Colab环境, 可以通过 Edit > Notebook Settings 来分配一个GPU使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a9b0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断当前环境GPU是否可用, 然后将tensor导入GPU内运行\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a92c7",
   "metadata": {},
   "source": [
    "#### 1. 张量的索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fbf9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[0,1] = 0            # 将第1行第1列的数据赋值为0\n",
    "print(tensor)\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0            # 将第1列(从0开始)的数据全部赋值为0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81671e89",
   "metadata": {},
   "source": [
    "#### 2. 张量的拼接\n",
    "可以通过torch.cat方法将一组张量按照指定的维度进行拼接, 也可以参考torch.stack方法。这个方法也可以实现拼接操作, 但和torch.cat稍微有点不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93175d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# dim指定维度\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90146f",
   "metadata": {},
   "source": [
    "#### 3. 张量的乘积和矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f363083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor): \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor: \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 逐个元素相乘结果\n",
    "print(f\"tensor.mul(tensor): \\n {tensor.mul(tensor)} \\n\")\n",
    "# 等价写法:\n",
    "print(f\"tensor * tensor: \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef9ff8",
   "metadata": {},
   "source": [
    "下面写法表示张量与张量的矩阵乘法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c837e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.T:\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor.matmul(tensor.T): \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T: \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor.T是tensor的转置\n",
    "print(\"tensor.T:\\n\",tensor.T)\n",
    "print(f\"tensor.matmul(tensor.T): \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# 等价写法:\n",
    "print(f\"tensor @ tensor.T: \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19705117",
   "metadata": {},
   "source": [
    "#### 4.自动赋值运算\n",
    "自动赋值运算通常在方法后有 _ 作为后缀, 例如: x.copy_(y), x.t_()操作会改变 x 的取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a06824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[56., 55., 56., 56.],\n",
      "        [56., 55., 56., 56.],\n",
      "        [56., 55., 56., 56.],\n",
      "        [56., 55., 56., 56.]]) \n",
      "\n",
      "tensor([[61., 60., 61., 61.],\n",
      "        [61., 60., 61., 61.],\n",
      "        [61., 60., 61., 61.],\n",
      "        [61., 60., 61., 61.]])\n"
     ]
    }
   ],
   "source": [
    "# 每次直接操作变量改变变量值\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2322f",
   "metadata": {},
   "source": [
    "### Tensor与Numpy的转化\n",
    "张量和Numpy array数组在CPU上可以共用一块内存区域, 改变其中一个另一个也会随之改变。 \n",
    "1. 由张量变换为Numpy array数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39bb938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea58c9",
   "metadata": {},
   "source": [
    "修改张量的值，则Numpy array数组值也会随之改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1d686da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1856552",
   "metadata": {},
   "source": [
    "2. 由Numpy array数组转为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2039342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = numpy.ones(5)\n",
    "print(\"n:\",n)\n",
    "t = torch.from_numpy(n)\n",
    "print(\"t:\",t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fca99",
   "metadata": {},
   "source": [
    "修改Numpy array数组的值，则张量值也会随之改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4236f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "numpy.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9f0c9",
   "metadata": {},
   "source": [
    "## 2.torch.autograd的简要介绍\n",
    "### 背景\n",
    "神经网络（NN）是在某些输入数据上执行的嵌套函数的集合。 这些函数由参数（由权重和偏差组成）定义，这些参数在 PyTorch 中存储在张量中。\n",
    "\n",
    "训练 NN 分为两个步骤：\n",
    "* **正向传播**：在正向传播中，NN 对正确的输出进行最佳猜测。 它通过其每个函数运行输入数据以进行猜测。\n",
    "* **反向传播**：在反向传播中，NN 根据其猜测中的误差调整其参数。 它通过从输出向后遍历，收集有关函数参数（梯度）的误差导数并使用梯度下降来优化参数来实现。\n",
    "### 在 PyTorch 中的用法\n",
    "让我们来看一个训练步骤。 对于此示例，我们从torchvision加载了经过预训练的 resnet18 模型。 我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&宽度为 64，其对应的label初始化为一些随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fff23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67ba12",
   "metadata": {},
   "source": [
    "接下来，我们通过模型的每一层运行输入数据以进行预测。 这是**正向传播**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874f10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352026d",
   "metadata": {},
   "source": [
    "我们使用模型的预测和相应的标签来计算误差（loss）。 下一步是通过网络反向传播此误差。 当我们在误差张量上调用.backward()时，开始反向传播。 然后，Autograd 会为每个模型参数计算梯度并将其存储在参数的.grad属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2bc10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.sum()所有元素求和\n",
    "loss = (prediction - labels).sum()\n",
    "#Pytorch对Tensor进行 backward() 自动求导时 需要设置记录（默认为Faslse） 才能反向传播\n",
    "loss.requires_grad_(True)\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe03e3e",
   "metadata": {},
   "source": [
    "接下来，我们加载一个优化器，在本例中为 SGD，学习率为 0.01，动量为 0.9。 我们在优化器中注册模型的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a031e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02036f",
   "metadata": {},
   "source": [
    "最后，我们调用.step()启动梯度下降。 优化器通过.grad中存储的梯度来调整每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e60ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72de82",
   "metadata": {},
   "source": [
    "### Autograd 的微分\n",
    "让我们来看看autograd如何收集梯度。 我们用requires_grad=True创建两个张量a和b。 这向autograd发出信号，应跟踪对它们的所有操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be83f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82576e3",
   "metadata": {},
   "source": [
    "                                                        我们从a和b创建另一个张量Q。\n",
    "![](https://pytorch.apachecn.org/docs/1.7/img/tex4-1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "350e45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8cdbfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40a8bf90",
   "metadata": {},
   "source": [
    "## 3.神经网络\n",
    "可以使用torch.nn包构建神经网络。\n",
    "\n",
    "现在您已经了解了autograd，nn依赖于autograd来定义模型并对其进行微分。 nn.Module包含层，以及返回output的方法forward(input)。\n",
    "卷积网\n",
    "\n",
    "这是一个简单的前馈网络。 它获取输入，将其一层又一层地馈入，然后最终给出输出。\n",
    "\n",
    "神经网络的典型训练过程如下：\n",
    "\n",
    "1. 定义具有一些可学习参数（或权重）的神经网络\n",
    "2. 遍历输入数据集\n",
    "3. 通过网络处理输入\n",
    "4. 计算损失（输出正确的距离有多远）\n",
    "5. 将梯度传播回网络参数\n",
    "通常使用简单的更新规则来更新网络的权重：weight = weight - learning_rate * gradient\n",
    "### 定义网络\n",
    "让我们定义这个网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02666b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
